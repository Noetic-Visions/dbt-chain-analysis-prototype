**Standard Operating Procedure: Revision of Agent-Generated Plans**

1.  **Receive Initial Plan Artifact:** A plan, typically structured in Markdown format, is generated by the AI agent and saved within the project's repository, often in a designated directory such as `/plans`. This artifact serves as a "runnable program".
2.  **Acknowledge Inherent Error:** Understand and accept the empirical fact that the plan, upon its initial generation, contains errors. Its appearance of completeness is misleading.
3.  **Human Verification - Comprehensive Review:** The human operator must **read the entire plan**. This step is non-negotiable and precedes any attempt at execution.
4.  **Error Identification and Classification:** During the review, identify discrepancies between the proposed plan and the desired outcome, accounting for architectural constraints, data formats, and implementation requirements of the existing codebase. Classify errors into two categories:
    *   **Simple, Localized Errors:** Inaccuracies, extraneous sections, or straightforward mistakes that are confined to specific parts of the plan.
    *   **Widespread or Complex Issues:** Problems that involve incorrect techniques, data formats, or have implications spanning numerous steps or reflecting fundamental misunderstandings of the architecture.
5.  **Execute Revision Based on Error Classification:**
    *   **For Simple Errors:** **Manually edit the Markdown plan file** directly. It is crucial to **avoid attempting to "lecture the LLM"** about these simple errors, as this action gratuitously **adds more context** to the prompt chain, counter-intuitively increasing the probability of further deviation and failure. This is akin to introducing noise into a system; it does not enhance understanding but obscures the desired signal.
    *   **For Widespread or Complex Issues:** **Instruct the agent to revise the entire plan**. Provide specific, clear feedback detailing the nature of the widespread issues (e.g., "incorrect data formats," "architectural technique is wrong," "implications across steps are flawed"). Allow the agent to regenerate or modify the plan based on this feedback.
6.  **Iterative Verification:** Following any revision, whether manual or agent-led, the human operator **must again read the entire plan** to verify that the corrections have been applied accurately and that no new errors have been introduced. This step is repeated as necessary.
7.  **Commit Revisions:** Save the revised plan file(s) and commit them to the repository with clear, human-readable messages detailing the nature of the changes. These commits serve as "commit breadcrumbs," documenting the evolution of the plan.
8.  **Assess Underlying Codebase Issues (Optional but Recommended):** Note recurrent difficulties the agent exhibits in generating or revising a correct plan. These difficulties frequently serve as indicators of "ugly truths" within the human-written codebase itself, highlighting problematic architectural design or implementation details that impede the agent's ability to plan effectively. Such observations should be documented and may necessitate a separate, planned effort to refactor the codebase, potentially utilizing the agent for this task with a newly generated plan.
9.  **Repeat Cycle:** Continue the process of human review, error classification, revision (manual or agent-led), and verification until the plan is deemed sufficiently accurate and robust for execution. The necessity for multiple revisions is the norm, particularly when first adopting this methodology.

This procedure is necessitated by the nature of the tools and the absence of such formalized, revisable plans in prior programming methodologies. The outcome is a refined, reusable plan artifact that increases the probability of successful execution by the agent and provides a versioned record for future reference or modification. Engaging in this rigorous revision process transforms the abstract notion of an agent's task into a concrete, verifiable set of instructions, thereby enhancing control and increasing the likelihood of producing shippable code.