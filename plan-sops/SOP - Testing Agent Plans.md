**Standard Operating Procedure (SOP) for Testing and Executing AI Agent Plans**

**1.0 Purpose**
To define the systematic process for validating, controlling, and executing plans generated by AI agents for modifying or interacting with a codebase, ensuring rigor and mitigating risks associated with autonomous agent actions.

**2.0 Scope**
This procedure applies to all plans generated by AI agents intended to effect changes within a codebase or associated systems (e.g., configuration files, terminal operations). It is to be initiated upon completion of the Plan formulation phase.

**3.0 Procedure**

3.1 **Initial Plan Disposition:** Upon the AI agent completing the generation of a plan:
    *   **Do not** permit the agent to proceed immediately to autonomous execution ("Pedal to the Metal"). Halt the process.
    *   Consider saving the plan artifact (typically a Markdown file) in a designated location within the repository (e.g., `/plans` folder). This serves as documentation and can be referenced later as context ("mini-rules"). Commit this plan file to version control with a clear message.

3.2 **Plan Review and Assessment:** The human operator shall rigorously review the generated plan:
    *   Examine the sequence of steps proposed by the agent.
    *   Identify any steps that are deemed more appropriate for manual execution by the human operator (e.g., file manipulations, terminal commands). Note these for manual handling outside the agent's execution cycle.
    *   Compare the plan steps against the actual state and architecture of the existing codebase.

3.3 **Identify Code Deficiencies and Refactoring Needs:** During Plan Review (Step 3.2), be vigilant for "ugly truths" or architectural deficiencies in the existing *human-written* code that the agent's plan exposes or struggles with.
    *   Recognise that identifying such issues is a valuable diagnostic function of this process.

3.4 **Conditional Refactoring Branch:** If the review (Step 3.3) reveals necessary refactoring or cleanup of the existing codebase *before* the agent's plan can be effectively implemented:
    *   Determine if refactoring is feasible and desirable at this juncture.
    *   If proceeding with refactoring, initiate a **new, separate thread/process** specifically for this refactoring task.
    *   Generate a **new plan** dedicated solely to the required refactoring. This new plan creation process should itself follow the established planning procedures.
    *   Execute the refactoring plan following this SOP (Steps 3.5 onwards) within its separate thread.
    *   Only return to the original plan execution *after* the necessary refactoring is completed and verified.

3.5 **Controlled, Step-by-Step Execution:** If proceeding with the original plan (or after necessary refactoring), execute the plan steps under strict human control:
    *   Instruct the agent to execute **only one step** of the plan at a time. Explicitly specify the step number to be executed, regardless of its position in the agent's original sequence if a different order is desired.
    *   **Do not** attempt to correct the agent's understanding or revise the plan extensively mid-execution via chat or detailed explanations.

3.6 **Post-Step Commitment:** Immediately following the agent's reported completion of a single step:
    *   **Manually commit** the changes made to the codebase.
    *   Ensure commit messages are clear, indicating which plan and step number were executed (e.g., "Ran step 3 of @feature-x-plan.md"). These serve as "commit breadcrumbs" for traceability and rollback.

3.7 **Post-Step Validation:** Immediately following the manual commitment (Step 3.6):
    *   **Manually test** the outcome of the executed step. This includes running builds, executing tests, and manually verifying user-facing aspects if applicable. **Do not** ask the agent to test its own output.

3.8 **Conditional Failure Handling:** If the manual test (Step 3.7) reveals that the executed step resulted in an error or undesirable outcome:
    *   **Do not** immediately instruct the agent to fix the problem within the current thread.
    *   Initiate a **new, separate thread/process** to address the issue.
    *   Generate a **new plan** specifically for creating the fix. Provide the agent with detailed context, including error messages, console output, screenshots highlighting the problem, or relevant diagrams. This new fix plan must also adhere to the planning procedures.
    *   Execute the fix plan following this SOP (Steps 3.5 onwards) within its separate thread.
    *   Once the fix is successfully planned, executed, and verified, return to the original plan execution thread (Step 3.5), potentially rolling back the failed step's commit first if necessary.

3.9 **Iteration:** Repeat Steps 3.5 through 3.8 until all necessary steps of the plan have been successfully executed and validated.

3.10 **Completion:** The plan execution phase is complete once all designated steps have been successfully performed, committed, and validated individually.

**4.0 Rationale (Underlying Principles)**
The structure of this SOP is necessitated by the fundamental characteristics of large language models (LLMs) used by AI agents. These models operate by predicting text based on training data and prompt context, rather than accumulating knowledge or truly "understanding" code meaning or purpose in a human developer's sense. They are prone to errors, especially with custom code, and cannot reliably reproduce specific code. Manual control, granular execution, and iterative validation are thus essential controls to manage this inherent unpredictability and ensure the integrity of the codebase. This approach transforms the process from one of merely accepting agent output into one of forensic analysis, controlled application, and continuous verification, ultimately enhancing the human operator's understanding and control over the development process.